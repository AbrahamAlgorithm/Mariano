async def scrape_product_details(driver, csv_file):
    try:
        await asyncio.sleep(20)  # Wait for the page to load
        product_name = driver.find_element(By.CSS_SELECTOR, 'h1[data-testid="product-details-name"]').text
        unit = driver.find_element(By.CSS_SELECTOR, 'span#ProductDetails-sellBy-unit').text
        upc = driver.find_element(By.CSS_SELECTOR, 'span[data-testid="product-details-upc"]').text.replace("UPC: ", "")
        location = driver.find_element(By.CSS_SELECTOR, 'span[data-testid="product-details-location"]').text

        # Extract price
        price_element = driver.find_element(By.CSS_SELECTOR, 'mark.kds-Price-promotional')
        dollars = price_element.find_element(By.CSS_SELECTOR, 'span.kds-Price-promotional-dropCaps').text
        cents = price_element.find_element(By.CSS_SELECTOR, 'sup.kds-Price-superscript').text.replace(".", "")
        price = f"${dollars}.{cents}"

        # Save the data to a CSV file
        with open(csv_file, mode="a", newline="", encoding="utf-8") as file:
            writer = csv.writer(file)
            writer.writerow([product_name, unit, upc, location, price])

    except Exception as e:
        print(f"Error scraping product details: {e}")






# CSV file setup
csv_file = "products.csv"

# Ensure the CSV file has headers
async def initialize_csv():
    try:
        with open("products.csv", mode="w", newline="", encoding="utf-8") as file:
            writer = csv.writer(file)
            # Write the headers
            writer.writerow(["Product Name", "Unit", "UPC", "Location", "Price"])
            print("csv file created sucessfully!!!")
    except Exception as e:
        print(f"Error initializing CSV file: {e}")

# Function to append data to the CSV
async def save_to_csv(data):
    try:
        with open(csv_file, mode="a", newline="", encoding="utf-8") as file:
            writer = csv.writer(file)
            # Append the product data
            writer.writerow([data["product_name"], data["unit"], data["upc"], data["location"], data["price"]])
            print("Data saved to CSV.")
    except Exception as e:
        print(f"Error writing to CSV: {e}")



    
    
def save_links_to_csv(product_links, filename="product_links.csv"):
    try:
        with open(filename, 'w', newline='') as file:
            writer = csv.writer(file)
            writer.writerow(["Product Link"])
            for link in product_links:
                writer.writerow([link])
        print(f"Saved {len(product_links)} links to {filename}")
    except Exception as e:
        print(f"Error saving links to CSV: {e}")

# Asynchronous function to visit each link and fetch details
async def fetch_product_details(session, url):
    try:
        async with session.get(url) as response:
            if response.status == 200:
                html = await response.text()
                print(f"Successfully fetched product page: {url}")
                # Process the HTML (e.g., parse with BeautifulSoup or another parser)
                return {"url": url, "content": html}
            else:
                print(f"Failed to fetch {url}: Status {response.status}")
                return None
    except Exception as e:
        print(f"Error visiting {url}: {e}")
        return None